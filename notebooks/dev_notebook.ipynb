{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zit9uAnJmmsG"
   },
   "source": [
    "# **Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qKGJtDUwkWCo"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "Cmvb92WQlRU5",
    "outputId": "6670ae51-e230-4fce-bbb2-7a503274dfbf"
   },
   "outputs": [],
   "source": [
    "data_path = \"/Users/dylantadas/Desktop/spring 25/i494- capstone project/dataset/\"  # path to OULAD.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dIgvgEmcnDBR",
    "outputId": "8d691175-32e3-4ec8-edbb-1aaed4b3fbd5"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(os.path.join(data_path, \"OULAD.zip\"), 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVE6izHRk2d6",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **I. Data Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mcmm4N4HYRoV",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EZljkauzYcQH"
   },
   "outputs": [],
   "source": [
    "# define utilized columns\n",
    "demographic_cols = [\n",
    "    'id_student', 'code_module', 'code_presentation',  # identifiers\n",
    "    'gender', 'region', 'highest_education', 'imd_band', 'age_band',  # categorical features\n",
    "    'num_of_prev_attempts', 'studied_credits', 'disability'  # mumerical features\n",
    "]\n",
    "\n",
    "vle_cols = [\n",
    "    'id_student', 'code_module', 'code_presentation',  # identifiers\n",
    "    'date', 'sum_click',  # activity metrics\n",
    "    'activity_type'  # for feature creation\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ns2UHI1PZCWp"
   },
   "outputs": [],
   "source": [
    "def load_raw_datasets() -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Loads all CSV files from OULAD dataset with optimized memory usage.\"\"\"\n",
    "\n",
    "    # define column types to reduce memory\n",
    "    dtype_dict = {\n",
    "        'id_student': 'int32',\n",
    "        'code_module': 'category',\n",
    "        'code_presentation': 'category',\n",
    "        'gender': 'category',\n",
    "        'region': 'category',\n",
    "        'highest_education': 'category',\n",
    "        'imd_band': 'category',\n",
    "        'age_band': 'category',\n",
    "        'num_of_prev_attempts': 'int8',\n",
    "        'disability': 'category',\n",
    "        'date': 'int16',\n",
    "        'sum_click': 'int16',\n",
    "        'module_presentation_length': 'int16'\n",
    "    }\n",
    "\n",
    "    # define which columns needed from each file\n",
    "    dataset_files = {\n",
    "        'student_info': {\n",
    "            'file': 'studentInfo.csv',\n",
    "            'columns': [\n",
    "                'id_student', 'code_module', 'code_presentation',\n",
    "                'gender', 'region', 'highest_education', 'imd_band', 'age_band',\n",
    "                'num_of_prev_attempts', 'studied_credits', 'disability'\n",
    "            ]\n",
    "        },\n",
    "        'vle_interactions': {\n",
    "            'file': 'studentVle.csv',\n",
    "            'columns': [\n",
    "                'id_student', 'code_module', 'code_presentation',\n",
    "                'date', 'sum_click', 'id_site'\n",
    "            ]\n",
    "        },\n",
    "        'vle_materials': {\n",
    "            'file': 'vle.csv',\n",
    "            'columns': None\n",
    "        },\n",
    "        'assessments': {\n",
    "            'file': 'assessments.csv',\n",
    "            'columns': None\n",
    "        },\n",
    "        'student_assessments': {\n",
    "            'file': 'studentAssessment.csv',\n",
    "            'columns': None\n",
    "        },\n",
    "        'courses': {\n",
    "            'file': 'courses.csv',\n",
    "            'columns': [\n",
    "                'code_module',\n",
    "                'code_presentation',\n",
    "                'module_presentation_length'\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    datasets = {}\n",
    "    total_rows = 0\n",
    "\n",
    "    for key, file_info in dataset_files.items():\n",
    "        try:\n",
    "            # load csw with specified columns and optimized dtypes\n",
    "            df = pd.read_csv(\n",
    "                    os.path.join(data_path, file_info['file']),\n",
    "                    usecols=file_info['columns'],\n",
    "                    dtype={col: dtype_dict.get(col) for col in (file_info['columns'] or [])\n",
    "                           if col in dtype_dict}\n",
    "            )\n",
    "            \n",
    "            total_rows += len(df)\n",
    "            print(f\"Loaded {file_info['file']}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "            datasets[key] = df\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: {file_info['file']} not found\")\n",
    "            datasets[key] = None\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Error loading {file_info['file']}: {str(e)}\")\n",
    "            print(\"Loading all columns instead...\")\n",
    "            # fallback: load all columns if unmatched specified columns\n",
    "            df = pd.read_csv(file_info['file'])\n",
    "            total_rows += len(df)\n",
    "            print(f\"Loaded {file_info['file']}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "            datasets[key] = df\n",
    "\n",
    "    print(f\"\\nTotal rows loaded across all files: {total_rows:,}\")\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKi4CTEYAHGB",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SjWOIE5hBaIg"
   },
   "outputs": [],
   "source": [
    "def perform_automated_eda(datasets):\n",
    "    \"\"\"Performs automated EDA and displays results directly in console, identifying potential data quality issues.\"\"\"\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Analysis of {name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # dataset overview\n",
    "        print(\"\\nDataset Overview:\")\n",
    "        print(f\"Number of rows: {len(df)}\")\n",
    "        print(f\"Number of columns: {len(df.columns)}\")\n",
    "        print(\"\\nColumns and their data types:\")\n",
    "        print(df.dtypes)\n",
    "\n",
    "        # missing values\n",
    "        print(\"\\nMissing Values Analysis:\")\n",
    "        missing = df.isnull().sum()\n",
    "        if missing.any():\n",
    "            print(missing[missing > 0])\n",
    "        else:\n",
    "            print(\"No missing values found\")\n",
    "\n",
    "        # numerical analysis\n",
    "        numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "        if len(numerical_cols) > 0:\n",
    "            print(\"\\nNumerical Columns Summary:\")\n",
    "            print(df[numerical_cols].describe())\n",
    "\n",
    "        # categorical analysis\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        if len(categorical_cols) > 0:\n",
    "            print(\"\\nCategorical Columns Summary:\")\n",
    "            for col in categorical_cols:\n",
    "                print(f\"\\nDistribution of {col}:\")\n",
    "                print(df[col].value_counts(normalize=True).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_Zf6WtbdBydt"
   },
   "outputs": [],
   "source": [
    "def analyze_student_performance(datasets):\n",
    "    \"\"\"Analyzes student performance patterns, displays demographic-related assessment score insights.\"\"\"\n",
    "\n",
    "    student_data = datasets['student_info'].merge(\n",
    "        datasets['student_assessments'],\n",
    "        on='id_student'\n",
    "    )\n",
    "\n",
    "    print(\"\\nStudent Performance Analysis\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # overall score distribution\n",
    "    print(\"\\nScore Distribution:\")\n",
    "    print(student_data['score'].describe())\n",
    "\n",
    "    # performance by demographic groups\n",
    "    for column in ['gender', 'age_band', 'imd_band']:\n",
    "        print(f\"\\nAverage Score by {column}:\")\n",
    "        print(student_data.groupby(column, observed=False)['score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GYnGY9WfCH2S"
   },
   "outputs": [],
   "source": [
    "def analyze_engagement_patterns(datasets):\n",
    "    \"\"\"Examines student engagement in vle, displays patterns in interaction frequency and timing.\"\"\"\n",
    "\n",
    "    vle_data = datasets['vle_interactions']\n",
    "\n",
    "    print(\"\\nEngagement Pattern Analysis\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # overall engagement metrics\n",
    "    print(\"\\nDaily Interaction Summary:\")\n",
    "    daily_interactions = vle_data.groupby('id_student')['sum_click'].sum()\n",
    "    print(daily_interactions.describe())\n",
    "\n",
    "    # activity timing\n",
    "    print(\"\\nTemporal Distribution of Activities:\")\n",
    "    activity_timing = vle_data.groupby('date')['sum_click'].mean()\n",
    "    print(activity_timing.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4HX1qhrQ7CHb"
   },
   "outputs": [],
   "source": [
    "def run_eda_pipeline(datasets):\n",
    "    \"\"\"Executes a complete EDA pipeline, displaying insights about data quality, student performance, and engagement patterns directly in the console.\"\"\"\n",
    "\n",
    "    print(\"\\nStarting EDA...\")\n",
    "    perform_automated_eda(datasets)\n",
    "    analyze_student_performance(datasets)\n",
    "    analyze_engagement_patterns(datasets)\n",
    "\n",
    "    print(\"\\nEDA Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_eda_findings(datasets):\n",
    "    \"\"\"Documents key findings from EDA to inform modeling decisions.\"\"\"\n",
    "    \n",
    "    findings = {\n",
    "        \"demographic_insights\": [],\n",
    "        \"performance_patterns\": [],\n",
    "        \"engagement_patterns\": [],\n",
    "        \"data_quality_issues\": [],\n",
    "        \"modeling_implications\": []\n",
    "    }\n",
    "    \n",
    "    # demographic insights\n",
    "    student_data = datasets['student_info']\n",
    "    findings[\"demographic_insights\"].append(\n",
    "        f\"Age distribution: {student_data['age_band'].value_counts(normalize=True).to_dict()}\"\n",
    "    )\n",
    "    findings[\"demographic_insights\"].append(\n",
    "        f\"Gender distribution: {student_data['gender'].value_counts(normalize=True).to_dict()}\"\n",
    "    )\n",
    "    findings[\"demographic_insights\"].append(\n",
    "        f\"IMD band distribution: {student_data['imd_band'].value_counts(normalize=True).to_dict()}\"\n",
    "    )\n",
    "    \n",
    "    # performance patterns\n",
    "    if 'student_assessments' in datasets:\n",
    "        performance_data = datasets['student_info'].merge(\n",
    "            datasets['student_assessments'],\n",
    "            on='id_student'\n",
    "        )\n",
    "        \n",
    "        # age-performance relationship\n",
    "        age_perf = performance_data.groupby('age_band', observed=False)['score'].mean().to_dict()\n",
    "        findings[\"performance_patterns\"].append(f\"Age-performance relationship: {age_perf}\")\n",
    "        \n",
    "        # imd-performance relationship\n",
    "        imd_perf = performance_data.groupby('imd_band', observed=False)['score'].mean().to_dict()\n",
    "        findings[\"performance_patterns\"].append(f\"IMD-performance relationship: {imd_perf}\")\n",
    "    \n",
    "    # engagement patterns\n",
    "    if 'vle_interactions' in datasets:\n",
    "        vle_data = datasets['vle_interactions']\n",
    "        \n",
    "        # activity distribution over time\n",
    "        activity_time = vle_data.groupby('date')['sum_click'].mean()\n",
    "        findings[\"engagement_patterns\"].append(\n",
    "            f\"Peak engagement day: Day {activity_time.idxmax()} with {activity_time.max():.2f} avg clicks\"\n",
    "        )\n",
    "        \n",
    "        # student engagement variability\n",
    "        student_engagement = vle_data.groupby('id_student')['sum_click'].sum()\n",
    "        findings[\"engagement_patterns\"].append(\n",
    "            f\"Engagement variability: min={student_engagement.min()}, max={student_engagement.max()}, \" \n",
    "            f\"median={student_engagement.median()}, std={student_engagement.std():.2f}\"\n",
    "        )\n",
    "    \n",
    "    # data quality issues\n",
    "    for name, df in datasets.items():\n",
    "        missing = df.isnull().sum()\n",
    "        if missing.any():\n",
    "            findings[\"data_quality_issues\"].append(\n",
    "                f\"{name} has missing values: {missing[missing > 0].to_dict()}\"\n",
    "            )\n",
    "    \n",
    "    # modeling implications\n",
    "    findings[\"modeling_implications\"].append(\n",
    "        \"Age shows clear correlation with performance; age-specific models might be beneficial\"\n",
    "    )\n",
    "    findings[\"modeling_implications\"].append(\n",
    "        \"IMD band shows socioeconomic gradient in performance; fairness metrics needed\"\n",
    "    )\n",
    "    findings[\"modeling_implications\"].append(\n",
    "        \"High variability in engagement suggests temporal features will be critical\"\n",
    "    )\n",
    "    \n",
    "    return findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bU1D_XcTk2ab",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Cleaning and Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "T41fQpOlp5tV"
   },
   "outputs": [],
   "source": [
    "def clean_demographic_data(student_info: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Performs initial cleaning of demographic data, handles missing values, and standardizes formats.\"\"\"\n",
    "\n",
    "    cleaned_data = student_info.copy()\n",
    "\n",
    "    # standardize string columns\n",
    "    string_columns = ['gender', 'region', 'highest_education', 'imd_band', 'age_band']\n",
    "    for col in string_columns:\n",
    "        cleaned_data[col] = cleaned_data[col].str.strip().str.lower()\n",
    "\n",
    "    # handle missing values\n",
    "    cleaned_data['imd_band'] = cleaned_data['imd_band'].fillna('unknown')\n",
    "    cleaned_data['disability'] = cleaned_data['disability'].fillna('N')\n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TOFZU1_2p8iY"
   },
   "outputs": [],
   "source": [
    "def clean_vle_data(vle_interactions: pd.DataFrame,\n",
    "                   vle_materials: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Cleans and merges VLE interaction data, removes outliers/invalid entries.\"\"\"\n",
    "\n",
    "    # remove interactions with invalid click counts\n",
    "    cleaned_interactions = vle_interactions[vle_interactions['sum_click'] > 0]\n",
    "\n",
    "    # merge with materials data\n",
    "    merged_data = cleaned_interactions.merge(\n",
    "        vle_materials,\n",
    "        on=['id_site', 'code_module', 'code_presentation'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # remove entries with missing material types\n",
    "    merged_data = merged_data.dropna(subset=['activity_type'])\n",
    "\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "caT1q5BCp-fV"
   },
   "outputs": [],
   "source": [
    "def clean_assessment_data(assessments: pd.DataFrame,\n",
    "                         student_assessments: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Cleans assessment data, handling missing scores and invalid dates.\"\"\"\n",
    "\n",
    "    # remove invalid scores\n",
    "    valid_assessments = student_assessments[\n",
    "        (student_assessments['score'] >= 0) &\n",
    "        (student_assessments['score'] <= 100)\n",
    "    ]\n",
    "\n",
    "    # merge with assessment data\n",
    "    cleaned_data = valid_assessments.merge(\n",
    "        assessments,\n",
    "        on='id_assessment',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ZhH_yvokySek"
   },
   "outputs": [],
   "source": [
    "def validate_data_consistency(datasets: Dict[str, pd.DataFrame]) -> bool:\n",
    "    \"\"\"Validates consistency across datasets.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # check student IDs consistency across files\n",
    "        student_ids = set(datasets['student_info']['id_student'])\n",
    "        vle_ids = set(datasets['vle_interactions']['id_student'])\n",
    "        assessment_ids = set(datasets['student_assessments']['id_student'])\n",
    "\n",
    "        # ensure all students in VLE and assessments exist in student_info\n",
    "        if not (vle_ids.issubset(student_ids) and assessment_ids.issubset(student_ids)):\n",
    "            print(\"Warning: Some VLE or assessment records have unknown student IDs\")\n",
    "\n",
    "        # check module-presentation pairs consistency\n",
    "        modules_presentations = set(zip(datasets['courses']['code_module'],\n",
    "                                     datasets['courses']['code_presentation']))\n",
    "        student_modules = set(zip(datasets['student_info']['code_module'],\n",
    "                                datasets['student_info']['code_presentation']))\n",
    "\n",
    "        if not student_modules.issubset(modules_presentations):\n",
    "            print(\"Warning: Invalid module-presentation combinations found\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Validation failed: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJ2PHLCNAqfb",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Processing Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZ3a8p7pphR3",
    "outputId": "c7bf82bc-26d1-41d5-ce2c-b3208831401b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded studentInfo.csv: 32593 rows, 11 columns\n",
      "Loaded studentVle.csv: 10655280 rows, 6 columns\n",
      "Loaded vle.csv: 6364 rows, 6 columns\n",
      "Loaded assessments.csv: 206 rows, 6 columns\n",
      "Loaded studentAssessment.csv: 173912 rows, 5 columns\n",
      "Loaded courses.csv: 22 rows, 3 columns\n",
      "\n",
      "Total rows loaded across all files: 10,868,377\n"
     ]
    }
   ],
   "source": [
    "# load and validate datasets\n",
    "datasets = load_raw_datasets()\n",
    "if not validate_data_consistency(datasets):\n",
    "    raise ValueError(\"Data validation failed. Check the datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KX2YSQ7nC4ra",
    "outputId": "c4dc6439-e441-4953-ce78-c88347203358"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting EDA...\n",
      "\n",
      "==================================================\n",
      "Analysis of student_info\n",
      "==================================================\n",
      "\n",
      "Dataset Overview:\n",
      "Number of rows: 32593\n",
      "Number of columns: 11\n",
      "\n",
      "Columns and their data types:\n",
      "code_module             category\n",
      "code_presentation       category\n",
      "id_student                 int32\n",
      "gender                  category\n",
      "region                  category\n",
      "highest_education       category\n",
      "imd_band                category\n",
      "age_band                category\n",
      "num_of_prev_attempts        int8\n",
      "studied_credits            int64\n",
      "disability              category\n",
      "dtype: object\n",
      "\n",
      "Missing Values Analysis:\n",
      "imd_band    1111\n",
      "dtype: int64\n",
      "\n",
      "Numerical Columns Summary:\n",
      "       studied_credits\n",
      "count     32593.000000\n",
      "mean         79.758691\n",
      "std          41.071900\n",
      "min          30.000000\n",
      "25%          60.000000\n",
      "50%          60.000000\n",
      "75%         120.000000\n",
      "max         655.000000\n",
      "\n",
      "==================================================\n",
      "Analysis of vle_interactions\n",
      "==================================================\n",
      "\n",
      "Dataset Overview:\n",
      "Number of rows: 10655280\n",
      "Number of columns: 6\n",
      "\n",
      "Columns and their data types:\n",
      "code_module          category\n",
      "code_presentation    category\n",
      "id_student              int32\n",
      "id_site                 int64\n",
      "date                    int16\n",
      "sum_click               int16\n",
      "dtype: object\n",
      "\n",
      "Missing Values Analysis:\n",
      "No missing values found\n",
      "\n",
      "Numerical Columns Summary:\n",
      "            id_site\n",
      "count  1.065528e+07\n",
      "mean   7.383234e+05\n",
      "std    1.312196e+05\n",
      "min    5.267210e+05\n",
      "25%    6.735190e+05\n",
      "50%    7.300690e+05\n",
      "75%    8.770300e+05\n",
      "max    1.049562e+06\n",
      "\n",
      "==================================================\n",
      "Analysis of vle_materials\n",
      "==================================================\n",
      "\n",
      "Dataset Overview:\n",
      "Number of rows: 6364\n",
      "Number of columns: 6\n",
      "\n",
      "Columns and their data types:\n",
      "id_site                int64\n",
      "code_module           object\n",
      "code_presentation     object\n",
      "activity_type         object\n",
      "week_from            float64\n",
      "week_to              float64\n",
      "dtype: object\n",
      "\n",
      "Missing Values Analysis:\n",
      "week_from    5243\n",
      "week_to      5243\n",
      "dtype: int64\n",
      "\n",
      "Numerical Columns Summary:\n",
      "            id_site    week_from      week_to\n",
      "count  6.364000e+03  1121.000000  1121.000000\n",
      "mean   7.260991e+05    15.204282    15.214987\n",
      "std    1.283151e+05     8.792865     8.779806\n",
      "min    5.267210e+05     0.000000     0.000000\n",
      "25%    6.615928e+05     8.000000     8.000000\n",
      "50%    7.300965e+05    15.000000    15.000000\n",
      "75%    8.140162e+05    22.000000    22.000000\n",
      "max    1.077905e+06    29.000000    29.000000\n",
      "\n",
      "Categorical Columns Summary:\n",
      "\n",
      "Distribution of code_module:\n",
      "code_module\n",
      "FFF    0.309082\n",
      "DDD    0.268385\n",
      "BBB    0.181332\n",
      "CCC    0.065839\n",
      "AAA    0.064896\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribution of code_presentation:\n",
      "code_presentation\n",
      "2013J    0.278441\n",
      "2014B    0.262571\n",
      "2014J    0.262414\n",
      "2013B    0.196574\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribution of activity_type:\n",
      "activity_type\n",
      "resource     0.417976\n",
      "subpage      0.165776\n",
      "oucontent    0.156505\n",
      "url          0.139221\n",
      "forumng      0.030484\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "==================================================\n",
      "Analysis of assessments\n",
      "==================================================\n",
      "\n",
      "Dataset Overview:\n",
      "Number of rows: 206\n",
      "Number of columns: 6\n",
      "\n",
      "Columns and their data types:\n",
      "code_module           object\n",
      "code_presentation     object\n",
      "id_assessment          int64\n",
      "assessment_type       object\n",
      "date                 float64\n",
      "weight               float64\n",
      "dtype: object\n",
      "\n",
      "Missing Values Analysis:\n",
      "date    11\n",
      "dtype: int64\n",
      "\n",
      "Numerical Columns Summary:\n",
      "       id_assessment        date      weight\n",
      "count     206.000000  195.000000  206.000000\n",
      "mean    26473.975728  145.005128   20.873786\n",
      "std     10098.625521   76.001119   30.384224\n",
      "min      1752.000000   12.000000    0.000000\n",
      "25%     15023.250000   71.000000    0.000000\n",
      "50%     25364.500000  152.000000   12.500000\n",
      "75%     34891.750000  222.000000   24.250000\n",
      "max     40088.000000  261.000000  100.000000\n",
      "\n",
      "Categorical Columns Summary:\n",
      "\n",
      "Distribution of code_module:\n",
      "code_module\n",
      "FFF    0.252427\n",
      "BBB    0.203883\n",
      "DDD    0.169903\n",
      "GGG    0.145631\n",
      "CCC    0.097087\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribution of code_presentation:\n",
      "code_presentation\n",
      "2014J    0.276699\n",
      "2014B    0.276699\n",
      "2013J    0.257282\n",
      "2013B    0.189320\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribution of assessment_type:\n",
      "assessment_type\n",
      "TMA     0.514563\n",
      "CMA     0.368932\n",
      "Exam    0.116505\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "==================================================\n",
      "Analysis of student_assessments\n",
      "==================================================\n",
      "\n",
      "Dataset Overview:\n",
      "Number of rows: 173912\n",
      "Number of columns: 5\n",
      "\n",
      "Columns and their data types:\n",
      "id_assessment       int64\n",
      "id_student          int64\n",
      "date_submitted      int64\n",
      "is_banked           int64\n",
      "score             float64\n",
      "dtype: object\n",
      "\n",
      "Missing Values Analysis:\n",
      "score    173\n",
      "dtype: int64\n",
      "\n",
      "Numerical Columns Summary:\n",
      "       id_assessment    id_student  date_submitted      is_banked  \\\n",
      "count  173912.000000  1.739120e+05   173912.000000  173912.000000   \n",
      "mean    26553.803556  7.051507e+05      116.032942       0.010977   \n",
      "std      8829.784254  5.523952e+05       71.484148       0.104194   \n",
      "min      1752.000000  6.516000e+03      -11.000000       0.000000   \n",
      "25%     15022.000000  5.044290e+05       51.000000       0.000000   \n",
      "50%     25359.000000  5.852080e+05      116.000000       0.000000   \n",
      "75%     34883.000000  6.344980e+05      173.000000       0.000000   \n",
      "max     37443.000000  2.698588e+06      608.000000       1.000000   \n",
      "\n",
      "               score  \n",
      "count  173739.000000  \n",
      "mean       75.799573  \n",
      "std        18.798107  \n",
      "min         0.000000  \n",
      "25%        65.000000  \n",
      "50%        80.000000  \n",
      "75%        90.000000  \n",
      "max       100.000000  \n",
      "\n",
      "==================================================\n",
      "Analysis of courses\n",
      "==================================================\n",
      "\n",
      "Dataset Overview:\n",
      "Number of rows: 22\n",
      "Number of columns: 3\n",
      "\n",
      "Columns and their data types:\n",
      "code_module                   category\n",
      "code_presentation             category\n",
      "module_presentation_length       int16\n",
      "dtype: object\n",
      "\n",
      "Missing Values Analysis:\n",
      "No missing values found\n",
      "\n",
      "Student Performance Analysis\n",
      "==================================================\n",
      "\n",
      "Score Distribution:\n",
      "count    207092.000000\n",
      "mean         75.402459\n",
      "std          19.081310\n",
      "min           0.000000\n",
      "25%          65.000000\n",
      "50%          79.000000\n",
      "75%          89.000000\n",
      "max         100.000000\n",
      "Name: score, dtype: float64\n",
      "\n",
      "Average Score by gender:\n",
      "gender\n",
      "F    75.621989\n",
      "M    75.239043\n",
      "Name: score, dtype: float64\n",
      "\n",
      "Average Score by age_band:\n",
      "age_band\n",
      "0-35     74.551903\n",
      "35-55    77.253011\n",
      "55<=     79.572172\n",
      "Name: score, dtype: float64\n",
      "\n",
      "Average Score by imd_band:\n",
      "imd_band\n",
      "0-10%      72.104766\n",
      "10-20      73.339453\n",
      "20-30%     74.304187\n",
      "30-40%     74.883066\n",
      "40-50%     75.353351\n",
      "50-60%     75.510884\n",
      "60-70%     75.898840\n",
      "70-80%     75.838700\n",
      "80-90%     77.449577\n",
      "90-100%    77.739800\n",
      "Name: score, dtype: float64\n",
      "\n",
      "Engagement Pattern Analysis\n",
      "==================================================\n",
      "\n",
      "Daily Interaction Summary:\n",
      "count    26074.000000\n",
      "mean      1518.949873\n",
      "std       1935.994635\n",
      "min          1.000000\n",
      "25%        298.000000\n",
      "50%        824.000000\n",
      "75%       2018.000000\n",
      "max      28615.000000\n",
      "Name: sum_click, dtype: float64\n",
      "\n",
      "Temporal Distribution of Activities:\n",
      "count    295.000000\n",
      "mean       3.569109\n",
      "std        0.604796\n",
      "min        1.708464\n",
      "25%        3.378936\n",
      "50%        3.643977\n",
      "75%        3.943591\n",
      "max        4.715334\n",
      "Name: sum_click, dtype: float64\n",
      "\n",
      "EDA Complete.\n"
     ]
    }
   ],
   "source": [
    "# eda analysis\n",
    "run_eda_pipeline(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DOCUMENTED EDA FINDINGS ===\n",
      "\n",
      "DEMOGRAPHIC_INSIGHTS\n",
      "--------------------\n",
      "• Age distribution: {'0-35': 0.7039548369281747, '35-55': 0.2894179731844261, '55<=': 0.0066271898873991346}\n",
      "• Gender distribution: {'M': 0.5484306446169422, 'F': 0.4515693553830577}\n",
      "• IMD band distribution: {'20-30%': 0.11606632361349342, '30-40%': 0.11241344260212184, '10-20': 0.11168286639984754, '0-10%': 0.10517120894479386, '40-50%': 0.103424178895877, '50-60%': 0.0992313067784766, '60-70%': 0.0922749507655168, '70-80%': 0.09144908201511975, '80-90%': 0.08773267263833301, '90-100%': 0.08055396734642017}\n",
      "\n",
      "PERFORMANCE_PATTERNS\n",
      "--------------------\n",
      "• Age-performance relationship: {'0-35': 74.55190257051278, '35-55': 77.25301108206502, '55<=': 79.57217165149545}\n",
      "• IMD-performance relationship: {'0-10%': 72.1047655904467, '10-20': 73.33945278915694, '20-30%': 74.30418701692825, '30-40%': 74.88306611025698, '40-50%': 75.35335069618351, '50-60%': 75.51088425161387, '60-70%': 75.89883973894126, '70-80%': 75.83869970635207, '80-90%': 77.44957675828195, '90-100%': 77.7397995509064}\n",
      "\n",
      "ENGAGEMENT_PATTERNS\n",
      "-------------------\n",
      "• Peak engagement day: Day 236 with 4.72 avg clicks\n",
      "• Engagement variability: min=1, max=28615, median=824.0, std=1935.99\n",
      "\n",
      "DATA_QUALITY_ISSUES\n",
      "-------------------\n",
      "• student_info has missing values: {'imd_band': 1111}\n",
      "• vle_materials has missing values: {'week_from': 5243, 'week_to': 5243}\n",
      "• assessments has missing values: {'date': 11}\n",
      "• student_assessments has missing values: {'score': 173}\n",
      "\n",
      "MODELING_IMPLICATIONS\n",
      "---------------------\n",
      "• Age shows clear correlation with performance; age-specific models might be beneficial\n",
      "• IMD band shows socioeconomic gradient in performance; fairness metrics needed\n",
      "• High variability in engagement suggests temporal features will be critical\n"
     ]
    }
   ],
   "source": [
    "# document findings from eda for later reference\n",
    "eda_findings = document_eda_findings(datasets)\n",
    "\n",
    "# print structured eda findings\n",
    "print(\"\\n=== DOCUMENTED EDA FINDINGS ===\")\n",
    "for category, items in eda_findings.items():\n",
    "    print(f\"\\n{category.upper()}\")\n",
    "    print(\"-\" * len(category))\n",
    "    for item in items:\n",
    "        print(f\"• {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "3pHGz0GNC56d"
   },
   "outputs": [],
   "source": [
    "# clean datasets\n",
    "cleaned_demographics = clean_demographic_data(datasets['student_info'])\n",
    "cleaned_vle = clean_vle_data(datasets['vle_interactions'],\n",
    "                            datasets['vle_materials'])\n",
    "cleaned_assessments = clean_assessment_data(datasets['assessments'],\n",
    "                                          datasets['student_assessments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JsvjO_Pk2YV"
   },
   "source": [
    "# **II. Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Feature Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ScGcR8bVQTjr"
   },
   "outputs": [],
   "source": [
    "def create_demographic_features(cleaned_demographics: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transforms cleaned demographic data into model features. Preserves key identifier columns before transformations.\"\"\"\n",
    "\n",
    "    features = cleaned_demographics.copy()\n",
    "\n",
    "    # preserve key identifier columns\n",
    "    id_columns = ['id_student', 'code_module', 'code_presentation']\n",
    "    preserved_ids = features[id_columns].copy()\n",
    "\n",
    "    # create encoded categorical variables\n",
    "    categorical_columns = ['gender', 'region', 'highest_education',\n",
    "                         'imd_band', 'age_band']\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        # label encoding\n",
    "        features[f\"{col}_encoded\"] = pd.Categorical(features[col]).codes\n",
    "\n",
    "        # one-hot encoding for tree-based models\n",
    "        one_hot = pd.get_dummies(features[col],\n",
    "                                prefix=col,\n",
    "                                drop_first=True)\n",
    "        features = pd.concat([features, one_hot], axis=1)\n",
    "\n",
    "    # create educational background features\n",
    "    features['is_first_attempt'] = (features['num_of_prev_attempts'] == 0)\n",
    "    features['credit_density'] = features['studied_credits'] / features['num_of_prev_attempts'].clip(1)\n",
    "\n",
    "    # use preserved key column copies\n",
    "    for col in id_columns:\n",
    "        features[col] = preserved_ids[col]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "SN8q1-KPFVZE"
   },
   "outputs": [],
   "source": [
    "def create_temporal_features(cleaned_vle_data: pd.DataFrame,\n",
    "                           window_sizes: List[int]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Creates time-based engagement features from VLE data using multiple window sizes.\"\"\"\n",
    "\n",
    "    temporal_features = {}\n",
    "\n",
    "    for window_size in window_sizes:\n",
    "        # create time windows\n",
    "        cleaned_vle_data['window'] = cleaned_vle_data['date'] // window_size\n",
    "\n",
    "        # store grouping columns\n",
    "        group_cols = ['id_student', 'code_module', 'code_presentation', 'window']\n",
    "\n",
    "        # handle numeric aggregations\n",
    "        numeric_metrics = cleaned_vle_data.groupby(group_cols).agg({\n",
    "            'sum_click': ['sum', 'mean', 'std'],\n",
    "            'id_site': 'nunique'\n",
    "        })\n",
    "\n",
    "        # flatten column names\n",
    "        numeric_metrics.columns = [\n",
    "            f\"{col[0]}_{col[1]}\" if isinstance(col, tuple) else col\n",
    "            for col in numeric_metrics.columns\n",
    "        ]\n",
    "        numeric_metrics = numeric_metrics.reset_index()\n",
    "\n",
    "        # handle activity type aggregation\n",
    "        activity_counts = pd.pivot_table(\n",
    "            cleaned_vle_data,\n",
    "            index=group_cols,\n",
    "            columns='activity_type',\n",
    "            values='sum_click',\n",
    "            aggfunc='count',\n",
    "            fill_value=0\n",
    "        )\n",
    "\n",
    "        # rename activity columns and reset index\n",
    "        activity_counts.columns = [f'activity_{col}' for col in activity_counts.columns]\n",
    "        activity_counts = activity_counts.reset_index()\n",
    "\n",
    "        # merge dataframes\n",
    "        temporal_features[f\"window_{window_size}\"] = numeric_metrics.merge(\n",
    "            activity_counts,\n",
    "            on=group_cols,\n",
    "            validate='one_to_one'  # validates 1-1 merge\n",
    "        )\n",
    "\n",
    "    return temporal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "caMx4zMjQd9-"
   },
   "outputs": [],
   "source": [
    "def create_assessment_features(cleaned_assessment_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Creates assessment-based features.\"\"\"\n",
    "\n",
    "    # preserve key identifier structure\n",
    "    group_cols = ['id_student', 'code_module', 'code_presentation']\n",
    "\n",
    "    # calculate submission delay and weighted components\n",
    "    cleaned_assessment_data['submission_delay'] = (\n",
    "        cleaned_assessment_data['date_submitted'] - cleaned_assessment_data['date']\n",
    "    )\n",
    "    cleaned_assessment_data['score_weight_product'] = (\n",
    "        cleaned_assessment_data['score'] * cleaned_assessment_data['weight']\n",
    "    )\n",
    "\n",
    "    # group and aggregate all metrics\n",
    "    performance_metrics = cleaned_assessment_data.groupby(group_cols).agg({\n",
    "        'score': ['mean', 'std', 'min', 'max', 'count'],\n",
    "        'submission_delay': ['mean', 'std'],\n",
    "        'weight': 'sum',\n",
    "        'is_banked': 'sum',\n",
    "        'score_weight_product': 'sum'\n",
    "    }).reset_index()  # preserves original column names\n",
    "\n",
    "    # flatten column names except identifier columns unchanged\n",
    "    performance_metrics.columns = [\n",
    "        col[0] if col[0] in group_cols else f\"{col[0]}_{col[1]}\"\n",
    "        for col in performance_metrics.columns\n",
    "    ]\n",
    "\n",
    "    # calculate final metrics\n",
    "    performance_metrics['weighted_score'] = (\n",
    "        performance_metrics['score_weight_product_sum'] /\n",
    "        performance_metrics['weight_sum'].replace(0, np.nan)\n",
    "    ).fillna(0)\n",
    "\n",
    "    performance_metrics['submission_consistency'] = (\n",
    "        performance_metrics['submission_delay_std'] /\n",
    "        performance_metrics['submission_delay_mean'].replace(0, np.nan)\n",
    "    ).fillna(0)\n",
    "\n",
    "    # drop intermediate calculation columns\n",
    "    performance_metrics = performance_metrics.drop(\n",
    "        ['score_weight_product_sum', 'weight_sum'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "LSq5lo9Y0zHO"
   },
   "outputs": [],
   "source": [
    "def create_sequential_features(cleaned_vle_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Creates sequential features for the GRU/LSTM path, maintaining temporal ordering crucial for sequential models.\"\"\"\n",
    "\n",
    "    # sort by student and time\n",
    "    sequential_data = cleaned_vle_data.sort_values(['id_student', 'date'])\n",
    "\n",
    "    # create time-based features\n",
    "    sequential_data['time_since_last'] = sequential_data.groupby('id_student')['date'].diff()\n",
    "    sequential_data['cumulative_clicks'] = sequential_data.groupby('id_student')['sum_click'].cumsum()\n",
    "\n",
    "    # create activity transition features\n",
    "    sequential_data['prev_activity'] = sequential_data.groupby('id_student')['activity_type'].shift()\n",
    "\n",
    "    return sequential_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "SxUS0gxnZQn6"
   },
   "outputs": [],
   "source": [
    "def prepare_dual_path_features(demographic_features, temporal_features,\n",
    "                             assessment_features, sequential_features,\n",
    "                             chunk_size: int = 50000):\n",
    "    \"\"\"Prepares dual-path features with chunked processing.\"\"\"\n",
    "\n",
    "    # process static path in chunks\n",
    "    static_chunks = []\n",
    "    for chunk_start in range(0, len(demographic_features), chunk_size):\n",
    "        chunk_end = chunk_start + chunk_size\n",
    "        demo_chunk = demographic_features.iloc[chunk_start:chunk_end]\n",
    "\n",
    "        # merge chunk with assessment features\n",
    "        static_chunk = demo_chunk.merge(\n",
    "            assessment_features,\n",
    "            on=['id_student', 'code_module', 'code_presentation'],\n",
    "            how='inner'\n",
    "        )\n",
    "        static_chunks.append(static_chunk)\n",
    "\n",
    "    # combine static path chunks\n",
    "    static_path = pd.concat(static_chunks, ignore_index=True)\n",
    "\n",
    "    # process sequential path similarly\n",
    "    sequential_path = sequential_features.merge(\n",
    "        temporal_features['window_7'],\n",
    "        on=['id_student', 'code_module', 'code_presentation', 'window'],\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'static_path': static_path,\n",
    "        'sequential_path': sequential_path\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "qlPh0Ijn1ai8"
   },
   "outputs": [],
   "source": [
    "# create all features\n",
    "demographic_features = create_demographic_features(cleaned_demographics)\n",
    "temporal_features = create_temporal_features(cleaned_vle, window_sizes=[7, 14, 30])\n",
    "assessment_features = create_assessment_features(cleaned_assessments)\n",
    "sequential_features = create_sequential_features(cleaned_vle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "hbXUSo2vZTi3"
   },
   "outputs": [],
   "source": [
    "dual_path_features = prepare_dual_path_features(\n",
    "    demographic_features,\n",
    "    temporal_features,\n",
    "    assessment_features,\n",
    "    sequential_features,\n",
    "    chunk_size=50000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7Xd1LZuPPEI",
    "outputId": "de4c26bd-9ece-4b43-d7ce-d95b56727626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Static Path Features:\n",
      "Number of samples: 25820\n",
      "Features included: ['code_module', 'code_presentation', 'id_student', 'gender', 'region', 'highest_education', 'imd_band', 'age_band', 'num_of_prev_attempts', 'studied_credits', 'disability', 'gender_encoded', 'gender_m', 'region_encoded', 'region_east midlands region', 'region_ireland', 'region_london region', 'region_north region', 'region_north western region', 'region_scotland', 'region_south east region', 'region_south region', 'region_south west region', 'region_wales', 'region_west midlands region', 'region_yorkshire region', 'highest_education_encoded', 'highest_education_he qualification', 'highest_education_lower than a level', 'highest_education_no formal quals', 'highest_education_post graduate qualification', 'imd_band_encoded', 'imd_band_10-20', 'imd_band_20-30%', 'imd_band_30-40%', 'imd_band_40-50%', 'imd_band_50-60%', 'imd_band_60-70%', 'imd_band_70-80%', 'imd_band_80-90%', 'imd_band_90-100%', 'imd_band_unknown', 'age_band_encoded', 'age_band_35-55', 'age_band_55<=', 'is_first_attempt', 'credit_density', 'score_mean', 'score_std', 'score_min', 'score_max', 'score_count', 'submission_delay_mean', 'submission_delay_std', 'is_banked_sum', 'weighted_score', 'submission_consistency']\n",
      "\n",
      "Sequential Path Features:\n",
      "Number of samples: 9975445\n",
      "Features included: ['code_module', 'code_presentation', 'id_student', 'id_site', 'date', 'sum_click', 'activity_type', 'week_from', 'week_to', 'window', 'time_since_last', 'cumulative_clicks', 'prev_activity', 'sum_click_sum', 'sum_click_mean', 'sum_click_std', 'id_site_nunique', 'activity_dataplus', 'activity_dualpane', 'activity_externalquiz', 'activity_folder', 'activity_forumng', 'activity_glossary', 'activity_homepage', 'activity_htmlactivity', 'activity_oucollaborate', 'activity_oucontent', 'activity_ouelluminate', 'activity_ouwiki', 'activity_page', 'activity_questionnaire', 'activity_quiz', 'activity_repeatactivity', 'activity_resource', 'activity_sharedsubpage', 'activity_subpage', 'activity_url']\n",
      "\n",
      "Data Alignment Check:\n",
      "Number of unique students in static path: 23351\n",
      "Number of unique students in sequential path: 24959\n"
     ]
    }
   ],
   "source": [
    "# print features to console\n",
    "print(\"\\nStatic Path Features:\")\n",
    "print(f\"Number of samples: {len(dual_path_features['static_path'])}\")\n",
    "print(\"Features included:\", dual_path_features['static_path'].columns.tolist())\n",
    "\n",
    "print(\"\\nSequential Path Features:\")\n",
    "print(f\"Number of samples: {len(dual_path_features['sequential_path'])}\")\n",
    "print(\"Features included:\", dual_path_features['sequential_path'].columns.tolist())\n",
    "\n",
    "# set static and sequential features\n",
    "static_features = dual_path_features['static_path']\n",
    "sequential_features = dual_path_features['sequential_path']\n",
    "\n",
    "# verify our data alignment\n",
    "print(\"\\nData Alignment Check:\")\n",
    "print(f\"Number of unique students in static path: {static_features['id_student'].nunique()}\")\n",
    "print(f\"Number of unique students in sequential path: {sequential_features['id_student'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Dataset Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stratified_splits(dual_path_features, test_size=0.2, random_state=0):\n",
    "    \"\"\"Creates stratified train/test splits preserving demographic distributions.\"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # extract static path features for stratification\n",
    "    static_features = dual_path_features['static_path']\n",
    "    \n",
    "    # create stratification columns\n",
    "    static_features['strat_gender'] = static_features['gender']\n",
    "    static_features['strat_age'] = static_features['age_band']\n",
    "    static_features['strat_imd'] = static_features['imd_band'].apply(\n",
    "        lambda x: x if pd.notna(x) else 'unknown'\n",
    "    )\n",
    "    \n",
    "    # create combined stratification column\n",
    "    static_features['stratify_col'] = static_features['strat_gender'] + '_' + \\\n",
    "                                     static_features['strat_age'] + '_' + \\\n",
    "                                     static_features['strat_imd'].astype(str)\n",
    "    \n",
    "    # get unique student ids\n",
    "    all_student_ids = static_features['id_student'].unique()\n",
    "    \n",
    "    # create student-level stratified split\n",
    "    student_df = static_features[['id_student', 'stratify_col']].drop_duplicates()\n",
    "    \n",
    "    # perform stratified split\n",
    "    train_ids, test_ids = train_test_split(\n",
    "        student_df['id_student'],\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=student_df['stratify_col']\n",
    "    )\n",
    "    \n",
    "    # split static and sequential features\n",
    "    static_train = static_features[static_features['id_student'].isin(train_ids)]\n",
    "    static_test = static_features[static_features['id_student'].isin(test_ids)]\n",
    "    \n",
    "    sequential_train = dual_path_features['sequential_path'][\n",
    "        dual_path_features['sequential_path']['id_student'].isin(train_ids)\n",
    "    ]\n",
    "    sequential_test = dual_path_features['sequential_path'][\n",
    "        dual_path_features['sequential_path']['id_student'].isin(test_ids)\n",
    "    ]\n",
    "    \n",
    "    # verify stratification maintained demographic proportions\n",
    "    for col in ['gender', 'age_band', 'imd_band']:\n",
    "        print(f\"\\nDistribution of {col} in training set:\")\n",
    "        print(static_train[col].value_counts(normalize=True))\n",
    "        \n",
    "        print(f\"\\nDistribution of {col} in test set:\")\n",
    "        print(static_test[col].value_counts(normalize=True))\n",
    "    \n",
    "    return {\n",
    "        'static_train': static_train,\n",
    "        'static_test': static_test,\n",
    "        'sequential_train': sequential_train,\n",
    "        'sequential_test': sequential_test,\n",
    "        'train_ids': train_ids,\n",
    "        'test_ids': test_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating stratified train/test splits...\n",
      "\n",
      "Distribution of gender in training set:\n",
      "gender\n",
      "m    0.549799\n",
      "f    0.450201\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribution of gender in test set:\n",
      "gender\n",
      "m    0.553138\n",
      "f    0.446862\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribution of age_band in training set:\n",
      "age_band\n",
      "0-35     0.694336\n",
      "35-55    0.298409\n",
      "55<=     0.007256\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribution of age_band in test set:\n",
      "age_band\n",
      "0-35     0.694070\n",
      "35-55    0.298999\n",
      "55<=     0.006931\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribution of imd_band in training set:\n",
      "imd_band\n",
      "30-40%     0.107870\n",
      "20-30%     0.106177\n",
      "10-20      0.100808\n",
      "40-50%     0.098679\n",
      "50-60%     0.098534\n",
      "0-10%      0.093213\n",
      "60-70%     0.092633\n",
      "70-80%     0.092439\n",
      "80-90%     0.088134\n",
      "90-100%    0.082717\n",
      "unknown    0.038795\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribution of imd_band in test set:\n",
      "imd_band\n",
      "20-30%     0.107239\n",
      "30-40%     0.106854\n",
      "10-20      0.100308\n",
      "40-50%     0.099538\n",
      "50-60%     0.098575\n",
      "0-10%      0.095110\n",
      "60-70%     0.092607\n",
      "70-80%     0.090682\n",
      "80-90%     0.087601\n",
      "90-100%    0.082980\n",
      "unknown    0.038506\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Split sizes:\n",
      "Static train: 20673 samples, 18694 students\n",
      "Static test: 5194 samples, 4680 students\n",
      "Sequential train: 7937487 samples\n",
      "Sequential test: 1985906 samples\n"
     ]
    }
   ],
   "source": [
    "# create train/test splits with stratification across demographic features\n",
    "print(\"\\nCreating stratified train/test splits...\")\n",
    "data_splits = create_stratified_splits(dual_path_features, test_size=0.2, random_state=0)\n",
    "\n",
    "# access the splits\n",
    "static_train = data_splits['static_train']\n",
    "static_test = data_splits['static_test']\n",
    "sequential_train = data_splits['sequential_train']\n",
    "sequential_test = data_splits['sequential_test']\n",
    "\n",
    "# print split sizes\n",
    "print(f\"\\nSplit sizes:\")\n",
    "print(f\"Static train: {len(static_train)} samples, {static_train['id_student'].nunique()} students\")\n",
    "print(f\"Static test: {len(static_test)} samples, {static_test['id_student'].nunique()} students\")\n",
    "print(f\"Sequential train: {len(sequential_train)} samples\")\n",
    "print(f\"Sequential test: {len(sequential_test)} samples\")\n",
    "\n",
    "# save the train/test split IDs for future reference\n",
    "train_ids = data_splits['train_ids']\n",
    "test_ids = data_splits['test_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_target_variable(data):\n",
    "    \"\"\"Creates binary target variable from final_result column.\"\"\"\n",
    "    \n",
    "    # final result mapping (0: not at risk, 1: at risk)\n",
    "    risk_mapping = {\n",
    "        'pass': 0,\n",
    "        'distinction': 0,\n",
    "        'fail': 1,\n",
    "        'withdrawal': 1\n",
    "    }\n",
    "    \n",
    "    # convert to lowercase and map to binary target\n",
    "    return data['final_result'].str.lower().map(risk_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(X, y, n_estimators=100, random_state=0):\n",
    "    \"\"\"Analyzes feature importance using Random Forest classifier.\"\"\"\n",
    "        \n",
    "    # select numeric columns for importance analysis\n",
    "    numeric_cols = X.select_dtypes(include=['int64', 'float64', 'bool']).columns\n",
    "    X_numeric = X[numeric_cols]\n",
    "    \n",
    "    # train random forest classifier\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "    model.fit(X_numeric, y)\n",
    "    importances = model.feature_importances_ # feature importances\n",
    "    \n",
    "    # convert to df for visualization\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': numeric_cols,\n",
    "        'Importance': importances\n",
    "    })\n",
    "    feature_importance = feature_importance.sort_values('Importance', ascending=False) # sort by importance\n",
    "    \n",
    "    # plot feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importance from Random Forest')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_correlations(X, threshold=0.85):\n",
    "    \"\"\"Analyzes correlation between numeric features.\"\"\"\n",
    "    \n",
    "    # select numeric columns\n",
    "    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    X_numeric = X[numeric_cols]\n",
    "    \n",
    "    # calculate correlation matrix\n",
    "    corr_matrix = X_numeric.corr()\n",
    "    \n",
    "    # plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # iterate through correlation matrix to find highly correlated pairs\n",
    "    correlated_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                correlated_pairs.append({\n",
    "                    'Feature1': corr_matrix.columns[i],\n",
    "                    'Feature2': corr_matrix.columns[j],\n",
    "                    'Correlation': corr_matrix.iloc[i, j]\n",
    "                })\n",
    "    \n",
    "    # convert to df\n",
    "    if correlated_pairs:\n",
    "        return pd.DataFrame(correlated_pairs)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['Feature1', 'Feature2', 'Correlation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in final_result column if not already in static_features\n",
    "if 'final_result' not in static_features.columns:\n",
    "    print(\"Debug step, if shown\")\n",
    "    static_features = static_features.merge(\n",
    "        datasets['student_info'][['id_student', 'code_module', 'code_presentation', 'final_result']],\n",
    "        on=['id_student', 'code_module', 'code_presentation'],\n",
    "        how='left'\n",
    "    )\n",
    "else:\n",
    "    print(\"DEBUG STATEMENT: Delete if statement in final_result column.\")\n",
    "\n",
    "# prepare target variable\n",
    "y = prepare_target_variable(static_features)\n",
    "\n",
    "# Create feature matrix (exclude non-feature columns)\n",
    "exclude_cols = ['id_student', 'code_module', 'code_presentation', 'final_result', \n",
    "                'stratify_col', 'strat_gender', 'strat_age', 'strat_imd']\n",
    "X = static_features.drop(columns=[col for col in exclude_cols if col in static_features.columns])\n",
    "\n",
    "# Analyze feature importance\n",
    "importance_df = analyze_feature_importance(X, y, n_estimators=100)\n",
    "print(\"Top 10 most important features:\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# Analyze feature correlations\n",
    "corr_df = analyze_feature_correlations(X, threshold=0.85)\n",
    "if not corr_df.empty:\n",
    "    print(\"\\nHighly correlated feature pairs:\")\n",
    "    print(corr_df)\n",
    "else:\n",
    "    print(\"\\nNo feature pairs with correlation above threshold were found.\")\n",
    "\n",
    "# Identify low-importance features\n",
    "low_importance_features = importance_df[importance_df['Importance'] < 0.01]['Feature'].tolist()\n",
    "if low_importance_features:\n",
    "    print(f\"\\nLow importance features (below 0.01):\")\n",
    "    print(low_importance_features)\n",
    "    print(\"\\nConsider removing these features to simplify the model.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
